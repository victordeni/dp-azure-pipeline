# Differential Privacy Data Pipeline on Azure

## ğŸš€ Overview

This project implements an **end-to-end data pipeline on Microsoft Azure** to process user event data while **enforcing Differential Privacy (DP)** guarantees.

The pipeline follows a **Bronze / Silver / Gold** architecture and is fully containerized.  
It is designed to run as an **Azure Container Apps Job**, automatically triggered when new data arrives in the Bronze layer.

Key objectives:
- Build a scalable analytics pipeline
- Apply Differential Privacy using **PipelineDP**
- Ensure cloud-native execution (Docker, Azure Blob Storage, Spark)
- Respect data minimization and privacy-by-design principles

---

## ğŸ—ï¸ High-Level Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Source â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ JSON events
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure Blob Storage â”‚
â”‚  Bronze Container  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Trigger (Timer Function)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure Container Apps Job     â”‚
â”‚  - Docker                    â”‚
â”‚  - Spark                     â”‚
â”‚  - PipelineDP                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚
       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silver Layer â”‚  â”‚ Gold Layer   â”‚
â”‚ Clean Data   â”‚  â”‚ DP Analytics â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ dp_app/
â”‚   â”œâ”€â”€ bronze_reader.py      # Read & flatten raw JSON events
â”‚   â”œâ”€â”€ silver_layer.py       # Spark transformations (clean session data)
â”‚   â”œâ”€â”€ dp_analysis.py        # Baseline + Differential Privacy analytics
â”‚   â”œâ”€â”€ gold_writer.py        # Persist results & plots to Gold
â”‚   â”œâ”€â”€ spark_utils.py        # Spark session configuration
â”‚   â”œâ”€â”€ env.py                # Environment & Azure configuration
â”‚   â”œâ”€â”€ logging_utils.py      # Structured logging
â”‚   â””â”€â”€ main.py               # Pipeline orchestration
â”‚
â”œâ”€â”€ dp-trigger-controller/    # Azure Function (Timer-triggered)
â”‚
â”œâ”€â”€ Dockerfile                # Container image definition
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ main.py                   # Container entrypoint
â””â”€â”€ README.md




---

## ğŸ¥‰ Bronze Layer â€” Raw Data

- Stores **raw JSON event files**
- No transformation
- Immutable source of truth
- Triggers the pipeline when new data arrives

Example fields:
- device
- geoNetwork
- summary metrics

---

## ğŸ¥ˆ Silver Layer â€” Clean Data

Purpose:
- Normalize and clean data
- One row per session
- Typed, analytics-ready format

Technologies:
- Apache Spark
- Parquet storage

Example columns:
- continent
- country
- device
- browser
- num_pageviews
- total_time_on_page

---

## ğŸ¥‡ Gold Layer â€” Privacy-Preserving Analytics

Only **aggregated data** is exposed.

Implemented analyses:
- Number of sessions per continent (DP COUNT)
- Average session duration per device (DP MEAN)

Outputs:
- JSON result files
- Comparison plots (raw vs DP)

No user-level or session-level data is stored in Gold.

---

## ğŸ” Differential Privacy Design

Differential Privacy is enforced using **PipelineDP**.

Key constraints:
- Bounded user contribution
- Noise calibrated using Îµ (epsilon)
- Explicit value bounds for numerical metrics

Example configuration:

AggregateParams(
    metrics=[Metrics.MEAN],
    min_value=0,
    max_value=600_000,
    max_partitions_contributed=1,
    max_contributions_per_partition=1
)



## âš™ï¸ Deployment Workflow
Build the Docker image
docker build -t dp-pipeline .

Push to Azure Container Registry
docker tag dp-pipeline dpsimacr.azurecr.io/dp-pipeline:v1
docker push dpsimacr.azurecr.io/dp-pipeline:v1

Run the Container Apps Job
az containerapp job start \
  -name dp-pipeline-job \
  -resource-group rg-dp-sim


## ğŸ” Automation & Triggering

An Azure Function (Timer Trigger):

Periodically scans the Bronze container

Detects new event files

Triggers the Container Apps Job

Uses a checkpoint to avoid reprocessing data

## ğŸ§  What This Project Demonstrates

Cloud-native data engineering

Apache Spark in containers

Azure Blob Storage & Container Apps

Practical Differential Privacy

Secure, reproducible analytics pipelines

## ğŸ“Œ Possible Extensions

- CI/CD with GitHub Actions
- Advanced privacy budget accounting
- Schema evolution management
- Visualization dashboards (Power BI, Looker)